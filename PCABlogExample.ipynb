{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c014dd35",
   "metadata": {},
   "source": [
    "Following example from https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, MaxPooling2D, Dense, Conv2D, Conv2DTranspose, Reshape, UpSampling2D, Layer, InputSpec\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "from tensorflow.keras import regularizers, activations, initializers, constraints, Sequential\n",
    "\n",
    "from tensorflow.keras.constraints import UnitNorm, Constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import decomposition\n",
    "import scipy\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Model #, load_model\n",
    "# from keras.layers import Input, Dense, Layer, InputSpec\n",
    "# from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "# from keras import regularizers, activations, initializers, constraints, Sequential\n",
    "# from keras import backend as K\n",
    "# from keras.constraints import UnitNorm, Constraint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb93bb",
   "metadata": {},
   "source": [
    "### Generate and prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5\n",
    "cov = sklearn.datasets.make_spd_matrix(n_dim, random_state=None)\n",
    "mu = np.random.normal(0, 0.1, n_dim)\n",
    "n = 1000\n",
    "X = np.random.multivariate_normal(mu, cov, n)\n",
    "X_train, X_test = train_test_split(X, test_size=0.5, random_state=123)\n",
    "# Scale the data between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a63c6e",
   "metadata": {},
   "source": [
    "### Estimation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_train_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1f38b",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e741898",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d671a9d",
   "metadata": {},
   "source": [
    "### Baseline reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc3f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f30518",
   "metadata": {},
   "source": [
    "## Autoencoder Optimization\n",
    "\n",
    "#### 1. Custom Layer: Tied weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTied(Layer):\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 tied_to=None,\n",
    "                 **kwargs):\n",
    "        self.tied_to = tied_to\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        if self.tied_to is not None:\n",
    "            self.kernel = K.transpose(self.tied_to.kernel)\n",
    "            self._non_trainable_weights.append(self.kernel)\n",
    "        else:\n",
    "            self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                          initializer=self.kernel_initializer,\n",
    "                                          name='kernel',\n",
    "                                          regularizer=self.kernel_regularizer,\n",
    "                                          constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93e9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd599fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder and decoder weights are the same\n",
    "w_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3)\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[1].T, 3)\n",
    "print('Encoder weights\\n', w_encoder)\n",
    "print('Decoder weights\\n', w_decoder)\n",
    "\n",
    "#biases are different\n",
    "b_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[1]), 3)\n",
    "b_decoder = np.round(np.transpose(autoencoder.layers[1].get_weights()[0]), 3)\n",
    "print('Encoder bias\\n', b_encoder)\n",
    "print('Decoder bias\\n', b_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f740a41",
   "metadata": {},
   "source": [
    "#### 2. Custom Constraint: Weights Orthogonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25333ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsOrthogonalityConstraint (Constraint):\n",
    "    def __init__(self, encoding_dim, weightage = 1.0, axis = 0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "        self.axis = axis\n",
    "        \n",
    "    def weights_orthogonality(self, w):\n",
    "        if(self.axis==1):\n",
    "            w = K.transpose(w)\n",
    "        if(self.encoding_dim > 1):\n",
    "            m = K.dot(K.transpose(w), w) - tf.eye(self.encoding_dim)\n",
    "            return self.weightage * K.sqrt(K.sum(K.square(m)))\n",
    "        else:\n",
    "            m = K.sum(w ** 2) - 1.\n",
    "            return m\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.weights_orthogonality(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28201d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias=True, \n",
    "                kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0)\n",
    "               ) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 2))\n",
    "\n",
    "w_decoder = autoencoder.layers[1].get_weights()[0]\n",
    "print('Decoder weights dot product\\n', np.round(np.dot(w_decoder, w_decoder.T), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb083c",
   "metadata": {},
   "source": [
    "#### 3. Custom Constraint: Uncorrelated Encoded features.\n",
    "For uncorrelated features, we will impose penalty on the sum of off-diagonal elements of the encoded features covariance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncorrelatedFeaturesConstraint (Constraint):\n",
    "\n",
    "    def __init__(self, encoding_dim, weightage=1.0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "\n",
    "    def get_covariance(self, x):\n",
    "        x_centered_list = []\n",
    "\n",
    "        for i in range(self.encoding_dim):\n",
    "            x_centered_list.append(x[:, i] - K.mean(x[:, i]))\n",
    "\n",
    "        x_centered = tf.stack(x_centered_list)\n",
    "        covariance = K.dot(x_centered, K.transpose(x_centered)) / \\\n",
    "            tf.cast(x_centered.get_shape()[0], tf.float32)\n",
    "\n",
    "        return covariance\n",
    "\n",
    "    # Constraint penalty\n",
    "    def uncorrelated_feature(self, x):\n",
    "        if(self.encoding_dim <= 1):\n",
    "            return 0.0\n",
    "        else:\n",
    "            output = K.sum(K.square(\n",
    "                self.covariance - tf.math.multiply(self.covariance, tf.eye(self.encoding_dim))))\n",
    "            return output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.covariance = self.get_covariance(x)\n",
    "        return self.weightage * self.uncorrelated_feature(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, weightage = 1.)) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dc381",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "print('Encoded feature covariance\\n', np.round(np.cov(encoded_features.T), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f0876",
   "metadata": {},
   "source": [
    "#### 4. Constraint: Unit Norm.\n",
    "UnitNorm constraint is prebuilt in Keras. We will apply this constraint on both Encoder and Decoder layers. It is important to note that we keep axis=0 for Encoder layer and axis=1 for the Decoder layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True, kernel_constraint=UnitNorm(axis=1))\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e25546",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 3.\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)  # W' in Figure 3.\n",
    "print('Encoder weights norm, \\n', np.round(np.sum(w_encoder ** 2, axis = 1),3))\n",
    "print('Decoder weights norm, \\n', np.round(np.sum(w_decoder ** 2, axis = 1),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf629cb2",
   "metadata": {},
   "source": [
    "### Putting Everything Together\n",
    "Here we will put together the above properties. Depending on the problem, a certain combination of these properties will work better than others.\n",
    "Applying several constraints together can sometimes harm the estimation. For example, in the dataset used here, combining Tied Layer, Weight Orthogonality, and UnitNorm worked the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2355b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0), kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)\n",
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
